{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5843018a",
   "metadata": {},
   "source": [
    "# PLS model to predict NTP composition during IVT reaction\n",
    "\n",
    "Using FTIR and a pH sensor, we can monitor the NTP composition and concentration during the IVT reaction.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../assets/pls_gp_ntp_monitoring.png\" alt=\"PLS model\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb3d11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "# Function to apply Standard Normal Variate (SNV)\n",
    "def standard_normal_variate(X):\n",
    "    \"\"\"\n",
    "    Apply Standard Normal Variate (SNV) transformation row-wise.\n",
    "    \"\"\"\n",
    "    X_snv = (X - X.mean(axis=1, keepdims=True)) / X.std(axis=1, keepdims=True)\n",
    "    return X_snv\n",
    "\n",
    "\n",
    "CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "CONSIDER_REPLICATES = True\n",
    "REMOVE_OUTLIERS = False\n",
    "VISUALISE_PREPROCESSED_DATA = True\n",
    "VISUALISE_RESULTS = False\n",
    "\n",
    "# Get the data and drop the wavenumber column\n",
    "CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "# Get the data and drop the wavenumber column\n",
    "data = pd.read_csv(\n",
    "    os.path.join(\n",
    "        CURRENT_DIR, \"wiz-app\", \"data_sets\", \"240403 4 NTP combinations Graph Data.csv\"\n",
    "    )\n",
    ")\n",
    "wavenumber = data[\"Wavenumber (1/cm)\"].values\n",
    "data.drop(columns=[\"Wavenumber (1/cm)\"], inplace=True)\n",
    "\n",
    "# Create the X and Y arrays\n",
    "X_list = []\n",
    "Y_list = []\n",
    "last_col_name = \"\"\n",
    "for col in data.columns:\n",
    "    if \"Unnamed\" not in col:\n",
    "        X_list.append(data[col].values.tolist())\n",
    "        Y_list.append([float(val) for val in col.replace(\"'\", \".\").strip().split(\" \")])\n",
    "        last_col_name = col\n",
    "    else:\n",
    "        if last_col_name and CONSIDER_REPLICATES:\n",
    "            X_list.append(data[col].values.tolist())\n",
    "            Y_list.append(\n",
    "                [\n",
    "                    float(val)\n",
    "                    for val in last_col_name.replace(\"'\", \".\").strip().split(\" \")\n",
    "                ]\n",
    "            )\n",
    "X = np.array(X_list)\n",
    "Y = np.array(Y_list)\n",
    "\n",
    "# generate a dataframe with the clean X and Y data and save it\n",
    "# pd.DataFrame(data=np.hstack((X, Y))).to_csv(\n",
    "#     \"cleaned_data.csv\", index=False\n",
    "# )\n",
    "\n",
    "\n",
    "if VISUALISE_RESULTS:\n",
    "    df = pd.DataFrame(data={f\"sample {i}\": X[i, :] for i in range(X.shape[0])})\n",
    "    df.plot()\n",
    "    plt.show()\n",
    "\n",
    "# Apply Savitzky-Golay filter row-wise\n",
    "X_savgol = savgol_filter(X, window_length=11, polyorder=3, axis=1)\n",
    "\n",
    "# Apply Standard Normal Variate (SNV) row-wise\n",
    "X_snv = standard_normal_variate(X_savgol)\n",
    "\n",
    "\n",
    "X = X_snv\n",
    "\n",
    "if REMOVE_OUTLIERS:\n",
    "    for i in range(3):\n",
    "        max_diff = [0, 0]\n",
    "        for i in range(X.shape[0]):\n",
    "            sample = X[i, :]\n",
    "            diff = sample.max() - sample.min()\n",
    "            if diff > max_diff[0]:\n",
    "                max_diff[0] = diff\n",
    "                max_diff[1] = i\n",
    "        X = np.delete(X, max_diff[1], axis=0)\n",
    "        Y = np.delete(Y, max_diff[1], axis=0)\n",
    "\n",
    "if VISUALISE_PREPROCESSED_DATA:\n",
    "    df = pd.DataFrame(data={f\"sample {i}\": X[i, :] for i in range(X.shape[0])})\n",
    "    df.plot()\n",
    "    plt.title(\"Raw Samples\")\n",
    "    plt.show()\n",
    "\n",
    "    # Create a new figure\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Get a colormap\n",
    "    cmap = cm.get_cmap(\"viridis\")  # or any other colormap\n",
    "\n",
    "    # Plot each sample with color based on sample number\n",
    "    for i in range(X.shape[0]):\n",
    "        color = cmap(i / X.shape[0])\n",
    "        ax.plot(wavenumber, X[i, :], color=color)\n",
    "\n",
    "    plt.title(\"Samples After Processing\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Create a PLS model\n",
    "pls = PLSRegression()\n",
    "\n",
    "# Use GridSearchCV to find the best number of components using KFold cross-validation\n",
    "param_grid = {\"n_components\": list(range(1, 21))}\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(pls, param_grid, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best number of components\n",
    "best_n_components = grid_search.best_params_[\"n_components\"]\n",
    "print(f\"Best number of components: {best_n_components}\")\n",
    "\n",
    "# Fit the PLS model with the best number of components\n",
    "pls_best = PLSRegression(n_components=best_n_components)\n",
    "pls_best.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred = pls_best.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions and the test targets to original scale\n",
    "Y_pred_original = Y_pred\n",
    "Y_test_original = Y_test\n",
    "\n",
    "# Calculate mean squared error and root mean squared error for the predictions\n",
    "mse = mean_squared_error(Y_test_original, Y_pred_original)\n",
    "rmse = np.sqrt(mse)\n",
    "# Calculate R-squared for the predictions\n",
    "r2 = r2_score(Y_test_original, Y_pred_original)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"R-squared: {r2}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Visualise the results\n",
    "if VISUALISE_RESULTS:\n",
    "    number_of_samples = 4\n",
    "\n",
    "    # Display the prediction results\n",
    "    X_test_sample = X_test[:number_of_samples]\n",
    "    Y_test_sample = Y_test_original[:number_of_samples]\n",
    "    Y_pred_sample = Y_pred_original[:number_of_samples]\n",
    "\n",
    "    print(\"\\nSample Test Data (X):\")\n",
    "    print(X_test_sample)\n",
    "    print(\"\\nActual Concentrations (Y Test):\")\n",
    "    print(Y_test_sample)\n",
    "    print(\"\\nPredicted Concentrations (Y Pred):\")\n",
    "    print(Y_pred_sample)\n",
    "\n",
    "    components = [\"ATP\", \"UTP\", \"CTP\", \"GTP\"]\n",
    "    # x_labels = [\"Actual\", \"Predicted\"]\n",
    "    colors = [\"b\", \"g\", \"r\", \"c\"]\n",
    "\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(14, 20))\n",
    "    fig.suptitle(\"Actual vs Predicted Concentrations for 4 Samples\", fontsize=16)\n",
    "\n",
    "    for i in range(number_of_samples):\n",
    "        bar_width = 0.4\n",
    "        indices = np.arange(len(components))\n",
    "        actual_bars = axs[i].bar(\n",
    "            indices, Y_test_sample[i], bar_width, label=\"Actual\", color=colors\n",
    "        )\n",
    "        predicted_bars = axs[i].bar(\n",
    "            indices + bar_width,\n",
    "            Y_pred_sample[i],\n",
    "            bar_width,\n",
    "            label=\"Predicted\",\n",
    "            color=colors,\n",
    "            alpha=0.6,\n",
    "        )\n",
    "\n",
    "        axs[i].set_title(f\"Sample {i}\")\n",
    "        axs[i].set_xticks(indices + bar_width / 2)\n",
    "        axs[i].set_xticklabels(components)\n",
    "        axs[i].legend()\n",
    "\n",
    "    plt.tight_layout(rect=(0, 0.03, 1, 0.95))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9d2ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Predict new samples with the current model\n",
    "\"\"\"\n",
    "from scipy.signal import savgol_filter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Function to apply Standard Normal Variate (SNV)\n",
    "def standard_normal_variate(X):\n",
    "    \"\"\"\n",
    "    Apply Standard Normal Variate (SNV) transformation row-wise.\n",
    "    \"\"\"\n",
    "    X_snv = (X - X.mean(axis=1, keepdims=True)) / X.std(axis=1, keepdims=True)\n",
    "    return X_snv\n",
    "\n",
    "CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# =========================#\n",
    "#                          #\n",
    "#   DATA PREPROCESSING     #\n",
    "#                          #\n",
    "# =========================#\n",
    "\n",
    "\n",
    "# HISTORICAL\n",
    "# Get the data and drop the wavenumber column\n",
    "data = pd.read_csv(\n",
    "    os.path.join(\n",
    "        CURRENT_DIR, \"wiz-app\", \"data_sets\", \"240403 4 NTP combinations Graph Data.csv\"\n",
    "    )\n",
    ")\n",
    "\n",
    "wavenumber = data[\"Wavenumber (1/cm)\"].values\n",
    "data.drop(columns=[\"Wavenumber (1/cm)\"], inplace=True)\n",
    "\n",
    "# Create the X and Y arrays\n",
    "X_list = []\n",
    "Y_list = []\n",
    "last_col_name = \"\"\n",
    "for col in data.columns:\n",
    "    if \"Unnamed\" not in col:\n",
    "        X_list.append(data[col].values.tolist())\n",
    "        Y_list.append([float(val) for val in col.replace(\"'\", \".\").strip().split(\" \")])\n",
    "        last_col_name = col\n",
    "    else:\n",
    "        if last_col_name:\n",
    "            X_list.append(data[col].values.tolist())\n",
    "            Y_list.append(\n",
    "                [\n",
    "                    float(val)\n",
    "                    for val in last_col_name.replace(\"'\", \".\").strip().split(\" \")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "X_raw = np.array(X_list)\n",
    "X_savgol = savgol_filter(X_raw, window_length=11, polyorder=3, axis=1)\n",
    "X_snv = standard_normal_variate(X_savgol)\n",
    "X = X_snv\n",
    "Y = np.array(Y_list)\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df.to_csv(\"X.csv\", index=False)\n",
    "df.to_csv(\"Y.csv\", index=False)\n",
    "\n",
    "\n",
    "# NEW\n",
    "new_data = pd.read_csv(os.path.join(CURRENT_DIR, \"wiz-app\", \"data_sets\", \"new new.csv\"))\n",
    "wavenumber_of_new = new_data[\"Wavenumber (1/cm)\"].values\n",
    "new_data.drop(columns=[\"Wavenumber (1/cm)\"], inplace=True)\n",
    "\n",
    "X_list_new = []\n",
    "last_col_name_new = \"\"\n",
    "for col in new_data.columns:\n",
    "        X_list_new.append(new_data[col].values.tolist())\n",
    "        last_col_name_new = col\n",
    "X_raw_new = np.array(X_list_new)\n",
    "X_new_savgol = savgol_filter(X_raw_new, window_length=11, polyorder=3, axis=1)\n",
    "X_new = standard_normal_variate(X_new_savgol)\n",
    "\n",
    "\n",
    "# =========================#\n",
    "#                          #\n",
    "#   PLS REGRESSION MODEL   #\n",
    "#                          #\n",
    "# =========================#\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Create a PLS model\n",
    "pls = PLSRegression()\n",
    "\n",
    "# Use GridSearchCV to find the best number of components using KFold cross-validation\n",
    "param_grid = {\"n_components\": list(range(1, 21))}\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(pls, param_grid, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best number of components\n",
    "best_n_components = grid_search.best_params_[\"n_components\"]\n",
    "print(f\"Best number of components: {best_n_components}\")\n",
    "\n",
    "# Fit the PLS model with the best number of components\n",
    "pls_best = PLSRegression(n_components=best_n_components)\n",
    "pls_best.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred = pls_best.predict(X_new)\n",
    "\n",
    "print(Y_pred)\n",
    "\n",
    "\n",
    "# # =======================================================#\n",
    "# #                                                        #\n",
    "# #   LEARN THE RESIDUALS USING NON-NEGATIVE REGRESSION    #\n",
    "# #                                                        #\n",
    "# # =======================================================#\n",
    "\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "\n",
    "# Custom wrapper to enforce non-negative predictions\n",
    "class NonNegativePLS(PLSRegression):\n",
    "    def predict(self, X_train_nn):\n",
    "        Y_pred = super().predict(X_train_nn)\n",
    "        # Apply non-negative constraint\n",
    "        Y_pred_nn = np.array([nnls(np.eye(Y_pred.shape[1]), y)[0] for y in Y_pred])\n",
    "        return Y_pred_nn\n",
    "\n",
    "\n",
    "# Create a PLS model\n",
    "pls_nn = NonNegativePLS()\n",
    "\n",
    "# Use GridSearchCV to find the best number of components using KFold cross-validation\n",
    "param_grid = {\"n_components\": list(range(1, 21))}\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(pls_nn, param_grid, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best number of components\n",
    "best_n_components = grid_search.best_params_[\"n_components\"]\n",
    "print(f\"Best number of components: {best_n_components}\")\n",
    "\n",
    "# Fit the PLS model with the best number of components\n",
    "pls_nn_best = NonNegativePLS(n_components=best_n_components)\n",
    "pls_nn_best.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_nn = pls_nn_best.predict(X_new)\n",
    "\n",
    "print(Y_pred_nn)\n",
    "\n",
    "# # ============================================================#\n",
    "# #                                                             #\n",
    "# #   LEARN THE RESIDUALS OF NNLS-PLS USING RANDOM FOREST       #\n",
    "# #                                                             #\n",
    "# # ============================================================#\n",
    "\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = Y_train - pls_nn_best.predict(X_train)\n",
    "\n",
    "# Secondary Model: RandomForestRegressor to learn residuals\n",
    "residual_model_rf = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "residual_model_rf.fit(X_train, residuals)\n",
    "\n",
    "# Correction of PLS predictions\n",
    "Y_pred_test_pls = pls_nn_best.predict(X_new)\n",
    "Y_residuals_pred = residual_model_rf.predict(X_new)\n",
    "Y_pred_corrected_rf = Y_pred_test_pls + Y_residuals_pred\n",
    "\n",
    "print(Y_pred_corrected_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64574c01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Function to apply Standard Normal Variate (SNV)\n",
    "def standard_normal_variate(X):\n",
    "    \"\"\"\n",
    "    Apply Standard Normal Variate (SNV) transformation row-wise.\n",
    "    \"\"\"\n",
    "    X_snv = (X - X.mean(axis=1, keepdims=True)) / X.std(axis=1, keepdims=True)\n",
    "    return X_snv\n",
    "\n",
    "\n",
    "# =========================#\n",
    "#                          #\n",
    "#   DATA PREPROCESSING     #\n",
    "#                          #\n",
    "# =========================#\n",
    "\n",
    "CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "# Get the data and drop the wavenumber column\n",
    "data = pd.read_csv(\n",
    "    os.path.join(\n",
    "        CURRENT_DIR, \"wiz-app\", \"data_sets\", \"240403 4 NTP combinations Graph Data.csv\"\n",
    "    )\n",
    ")\n",
    "\n",
    "wavenumber = data[\"Wavenumber (1/cm)\"].values\n",
    "data.drop(columns=[\"Wavenumber (1/cm)\"], inplace=True)\n",
    "\n",
    "# Create the X and Y arrays\n",
    "X_list = []\n",
    "Y_list = []\n",
    "last_col_name = \"\"\n",
    "for col in data.columns:\n",
    "    if \"Unnamed\" not in col:\n",
    "        X_list.append(data[col].values.tolist())\n",
    "        Y_list.append([float(val) for val in col.replace(\"'\", \".\").strip().split(\" \")])\n",
    "        last_col_name = col\n",
    "    else:\n",
    "        if last_col_name:\n",
    "            X_list.append(data[col].values.tolist())\n",
    "            Y_list.append(\n",
    "                [\n",
    "                    float(val)\n",
    "                    for val in last_col_name.replace(\"'\", \".\").strip().split(\" \")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "X_raw = np.array(X_list)\n",
    "\n",
    "# Apply Savitzky-Golay filter row-wise\n",
    "X_savgol = savgol_filter(X_raw, window_length=11, polyorder=3, axis=1)\n",
    "\n",
    "# Apply Standard Normal Variate (SNV) row-wise\n",
    "X_snv = standard_normal_variate(X_savgol)\n",
    "\n",
    "X = X_snv\n",
    "Y = np.array(Y_list)\n",
    "\n",
    "# =========================#\n",
    "#                          #\n",
    "#   PLS REGRESSION MODEL   #\n",
    "#                          #\n",
    "# =========================#\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Create a PLS model\n",
    "pls = PLSRegression()\n",
    "\n",
    "# Use GridSearchCV to find the best number of components using KFold cross-validation\n",
    "param_grid = {\"n_components\": list(range(1, 21))}\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(pls, param_grid, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best number of components\n",
    "best_n_components = grid_search.best_params_[\"n_components\"]\n",
    "print(f\"Best number of components: {best_n_components}\")\n",
    "\n",
    "# Fit the PLS model with the best number of components\n",
    "pls_best = PLSRegression(n_components=best_n_components)\n",
    "pls_best.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred = pls_best.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error and root mean squared error for the predictions\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "# Calculate R-squared for the predictions\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"R-squared: {r2}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "number_of_samples = 4\n",
    "\n",
    "# Display the prediction results\n",
    "Y_test_sample = Y_test[:number_of_samples]\n",
    "Y_pred_sample = Y_pred[:number_of_samples]\n",
    "\n",
    "print(\"\\nActual Concentrations (Y Test):\")\n",
    "print(Y_test_sample)\n",
    "print(\"\\nPredicted Concentrations (Y Pred):\")\n",
    "print(Y_pred_sample)\n",
    "\n",
    "\n",
    "# =============================================#\n",
    "#                                              #\n",
    "#   LEARN THE RESIDUALS USING RANDOM FOREST    #\n",
    "#                                              #\n",
    "# =============================================#\n",
    "\n",
    "\n",
    "# Calculate residuals on the training set\n",
    "residuals = Y_train - pls_best.predict(X_train)\n",
    "\n",
    "# Secondary Model: RandomForestRegressor to learn residuals\n",
    "residual_model_rf = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "residual_model_rf.fit(X_train, residuals)\n",
    "\n",
    "# Correction of PLS predictions\n",
    "Y_pred_test_pls = pls_best.predict(X_test)\n",
    "Y_residuals_pred = residual_model_rf.predict(X_test)\n",
    "Y_pred_corrected_rf = Y_pred_test_pls + Y_residuals_pred\n",
    "\n",
    "\n",
    "mse_rf = mean_squared_error(Y_test, Y_pred_corrected_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(Y_test, Y_pred_corrected_rf)\n",
    "\n",
    "print(f\"RandomForest Corrected - R-squared: {r2_rf}, MSE: {mse_rf}, RMSE: {rmse_rf}\")\n",
    "\n",
    "number_of_samples = 4\n",
    "\n",
    "# Display the prediction results\n",
    "Y_test_sample = Y_test[:number_of_samples]\n",
    "Y_pred_sample = Y_pred_corrected_rf[:number_of_samples]\n",
    "\n",
    "print(\"\\nActual Concentrations (Y Test):\")\n",
    "print(Y_test_sample)\n",
    "print(\"\\nPredicted Concentrations (Y Pred):\")\n",
    "print(Y_pred_sample)\n",
    "\n",
    "\n",
    "number_of_samples = 4\n",
    "\n",
    "# Display the prediction results\n",
    "Y_test_sample = Y_test[:number_of_samples]\n",
    "Y_pred_sample = Y_pred_corrected_rf[:number_of_samples]\n",
    "\n",
    "print(\"\\nActual Concentrations (Y Test):\")\n",
    "print(Y_test_sample)\n",
    "print(\"\\nPredicted Concentrations (Y Pred):\")\n",
    "print(Y_pred_sample)\n",
    "\n",
    "\n",
    "# =======================================================#\n",
    "#                                                        #\n",
    "#   LEARN THE RESIDUALS USING NON-NEGATIVE REGRESSION    #\n",
    "#                                                        #\n",
    "# =======================================================#\n",
    "\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "\n",
    "# Custom wrapper to enforce non-negative predictions\n",
    "class NonNegativePLS(PLSRegression):\n",
    "    def predict(self, X_train_nn):\n",
    "        Y_pred = super().predict(X_train_nn)\n",
    "        # Apply non-negative constraint\n",
    "        Y_pred_nn = np.array([nnls(np.eye(Y_pred.shape[1]), y)[0] for y in Y_pred])\n",
    "        return Y_pred_nn\n",
    "\n",
    "\n",
    "# Create a PLS model\n",
    "pls_nn = NonNegativePLS()\n",
    "\n",
    "# Use GridSearchCV to find the best number of components using KFold cross-validation\n",
    "param_grid = {\"n_components\": list(range(1, 21))}\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(pls_nn, param_grid, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best number of components\n",
    "best_n_components = grid_search.best_params_[\"n_components\"]\n",
    "print(f\"Best number of components: {best_n_components}\")\n",
    "\n",
    "# Fit the PLS model with the best number of components\n",
    "pls_nn_best = NonNegativePLS(n_components=best_n_components)\n",
    "pls_nn_best.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_nn = pls_nn_best.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error and root mean squared error for the predictions\n",
    "mse = mean_squared_error(Y_test, Y_pred_nn)\n",
    "rmse = np.sqrt(mse)\n",
    "# Calculate R-squared for the predictions\n",
    "r2 = r2_score(Y_test, Y_pred_nn)\n",
    "\n",
    "print(f\"Non Negative Regression PLS - R-squared: {r2}, MSE: {mse}, RMSE: {rmse}\")\n",
    "\n",
    "number_of_samples = 4\n",
    "\n",
    "# Display the prediction results\n",
    "Y_test_sample = Y_test[:number_of_samples]\n",
    "Y_pred_sample = Y_pred_nn[:number_of_samples]\n",
    "\n",
    "print(\"\\nActual Concentrations (Y Test):\")\n",
    "print(Y_test_sample)\n",
    "print(\"\\nPredicted Concentrations (Y Pred):\")\n",
    "print(Y_pred_sample)\n",
    "\n",
    "\n",
    "# ============================================================#\n",
    "#                                                             #\n",
    "#   LEARN THE RESIDUALS OF NNLS-PLS USING RANDOM FOREST       #\n",
    "#                                                             #\n",
    "# ============================================================#\n",
    "\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = Y_train - pls_nn_best.predict(X_train)\n",
    "\n",
    "# Secondary Model: RandomForestRegressor to learn residuals\n",
    "residual_model_rf = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "residual_model_rf.fit(X_train, residuals)\n",
    "\n",
    "# Correction of PLS predictions\n",
    "Y_pred_test_pls = pls_nn_best.predict(X_test)\n",
    "Y_residuals_pred = residual_model_rf.predict(X_test)\n",
    "Y_pred_corrected_rf = Y_pred_test_pls + Y_residuals_pred\n",
    "\n",
    "\n",
    "mse_rf = mean_squared_error(Y_test, Y_pred_corrected_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(Y_test, Y_pred_corrected_rf)\n",
    "\n",
    "print(\n",
    "    f\"Non Negative Regression RandomForest Corrected - R-squared: {r2_rf}, MSE: {mse_rf}, RMSE: {rmse_rf}\"\n",
    ")\n",
    "\n",
    "number_of_samples = 4\n",
    "\n",
    "# Display the prediction results\n",
    "Y_test_sample = Y_test[:number_of_samples]\n",
    "Y_pred_sample = Y_pred_corrected_rf[:number_of_samples]\n",
    "\n",
    "print(\"\\nActual Concentrations (Y Test):\")\n",
    "print(Y_test_sample)\n",
    "print(\"\\nPredicted Concentrations (Y Pred):\")\n",
    "print(Y_pred_sample)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Set the global font to be DejaVu Sans, size 10 (or any other preferred font and size)\n",
    "plt.rcParams[\"font.size\"] = \"10\"\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "\n",
    "\n",
    "def round_to_sig_figs(num, sig_figs):\n",
    "    \"\"\"\n",
    "    Rounds a number to a specified number of significant figures.\n",
    "\n",
    "    Parameters:\n",
    "    - num: The number to be rounded.\n",
    "    - sig_figs: The number of significant figures to round to.\n",
    "\n",
    "    Returns:\n",
    "    - Rounded number to the specified significant figures.\n",
    "    \"\"\"\n",
    "    if num == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return round(num, sig_figs - int(np.floor(np.log10(abs(num)))) - 1)\n",
    "\n",
    "\n",
    "# Calculate MAE for each model\n",
    "mae_pls = round_to_sig_figs(mean_absolute_error(Y_test, Y_pred), 2)\n",
    "mae_rf_corrected_pls = round_to_sig_figs(\n",
    "    mean_absolute_error(Y_test, Y_pred_corrected_rf), 2\n",
    ")\n",
    "mae_nn_pls = round_to_sig_figs(mean_absolute_error(Y_test, Y_pred_nn), 2)\n",
    "mae_rf_corrected_nn_pls = round_to_sig_figs(\n",
    "    mean_absolute_error(Y_test, Y_pred_corrected_rf), 2\n",
    ")\n",
    "\n",
    "# Collect metrics for each model\n",
    "metrics = {\n",
    "    \"PLS\": {\n",
    "        \"RMSE\": round_to_sig_figs(rmse, 2),\n",
    "        \"R²\": round_to_sig_figs(r2, 2),\n",
    "        \"MAE\": mae_pls,\n",
    "    },\n",
    "    \"RF Corrected PLS\": {\n",
    "        \"RMSE\": round_to_sig_figs(rmse_rf, 2),\n",
    "        \"R²\": round_to_sig_figs(r2_rf, 2),\n",
    "        \"MAE\": mae_rf_corrected_pls,\n",
    "    },\n",
    "    \"Non-Neg PLS\": {\n",
    "        \"RMSE\": round_to_sig_figs(rmse, 2),\n",
    "        \"R²\": round_to_sig_figs(r2, 2),\n",
    "        \"MAE\": mae_nn_pls,\n",
    "    },\n",
    "    \"RF Corrected Non-Neg PLS\": {\n",
    "        \"RMSE\": round_to_sig_figs(rmse_rf, 2),\n",
    "        \"R²\": round_to_sig_figs(r2_rf, 2),\n",
    "        \"MAE\": mae_rf_corrected_nn_pls,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Extract data for plotting\n",
    "models = list(metrics.keys())\n",
    "rmse_values = [metrics[model][\"RMSE\"] for model in models]\n",
    "r2_values = [metrics[model][\"R²\"] for model in models]\n",
    "mae_values = [metrics[model][\"MAE\"] for model in models]\n",
    "\n",
    "# Plot RMSE, R², and MAE in a horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "y = np.arange(len(models))  # the label locations\n",
    "height = 0.2  # the height of the bars\n",
    "\n",
    "rects1 = ax.barh(y - height, rmse_values, height, label=\"RMSE\")\n",
    "rects2 = ax.barh(y, r2_values, height, label=\"R²\")\n",
    "rects3 = ax.barh(y + height, mae_values, height, label=\"MAE\")\n",
    "\n",
    "# Add some text for labels, title and axes ticks\n",
    "ax.set_ylabel(\"Model\")\n",
    "ax.set_title(\"Comparison of RMSE, R², and MAE for Different Models\")\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(models)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "ax.bar_label(rects3, padding=3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Predicted vs. Measured for the best model\n",
    "best_model_name = (\n",
    "    \"RF Corrected Non-Neg PLS\"  # Assuming this is the best based on metrics\n",
    ")\n",
    "Y_pred_best = Y_pred_corrected_rf  # Replace with the best model's predictions\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(Y_test.flatten(), Y_pred_best.flatten(), edgecolors=(0, 0, 0))\n",
    "plt.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], \"k--\", lw=4)\n",
    "plt.xlabel(\"Measured NTP Composition\")\n",
    "plt.ylabel(\"Predicted NTP Composition\")\n",
    "plt.title(f\"Predicted vs Measured NTP composition for {best_model_name}\")\n",
    "\n",
    "\n",
    "number_of_samples = 4\n",
    "\n",
    "# Display the prediction results\n",
    "Y_test_sample = [\n",
    "    Y_test[:1][0],\n",
    "    Y_test[:1][0],\n",
    "    Y_test[:1][0],\n",
    "    Y_test[:1][0],\n",
    "]\n",
    "Y_pred_sample = [\n",
    "    [Y_pred[:1][0], \"PLS\"],\n",
    "    [Y_pred_corrected_rf[:1][0], \"RF Corrected PLS\"],\n",
    "    [Y_pred_nn[:1][0], \"Non-Neg PLS\"],\n",
    "    [Y_pred_corrected_rf[:1][0], \"RF Corrected Non-Neg PLS\"],\n",
    "]\n",
    "\n",
    "\n",
    "components = [\"ATP\", \"UTP\", \"CTP\", \"GTP\"]\n",
    "# x_labels = [\"Actual\", \"Predicted\"]\n",
    "colors = [\"b\", \"g\", \"r\", \"c\"]\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=(14, 20))\n",
    "fig.suptitle(\"Actual vs Predicted Concentrations for 4 Samples\", fontsize=16)\n",
    "\n",
    "for i in range(number_of_samples):\n",
    "    bar_width = 0.4\n",
    "    indices = np.arange(len(components))\n",
    "    actual_bars = axs[i].bar(\n",
    "        indices, Y_test_sample[i], bar_width, label=\"Actual\", color=colors\n",
    "    )\n",
    "    predicted_bars = axs[i].bar(\n",
    "        indices + bar_width,\n",
    "        Y_pred_sample[i][0],\n",
    "        bar_width,\n",
    "        label=f\"{Y_pred_sample[i][1]} Prediction\",\n",
    "        color=colors,\n",
    "        alpha=0.6,\n",
    "    )\n",
    "\n",
    "    axs[i].set_title(\"Sample OOD\")\n",
    "    axs[i].set_xticks(indices + bar_width / 2)\n",
    "    axs[i].set_xticklabels(components)\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout(rect=(0, 0.03, 1, 0.95))\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b62f44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Function to apply Standard Normal Variate (SNV)\n",
    "def standard_normal_variate(X):\n",
    "    \"\"\"\n",
    "    Apply Standard Normal Variate (SNV) transformation row-wise.\n",
    "    \"\"\"\n",
    "    X_snv = (X - X.mean(axis=1, keepdims=True)) / X.std(axis=1, keepdims=True)\n",
    "    return X_snv\n",
    "\n",
    "\n",
    "# =========================#\n",
    "#                          #\n",
    "#   DATA PREPROCESSING     #\n",
    "#                          #\n",
    "# =========================#\n",
    "\n",
    "CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "# Get the data and drop the wavenumber column\n",
    "data = pd.read_csv(\n",
    "    os.path.join(CURRENT_DIR, \"wiz-app\", \"data_sets\", \"240403 4 NTP combinations Graph Data.csv\")\n",
    ")\n",
    "wavenumber = data[\"Wavenumber (1/cm)\"].values\n",
    "data.drop(columns=[\"Wavenumber (1/cm)\"], inplace=True)\n",
    "\n",
    "# Create the X and Y arrays\n",
    "X_list = []\n",
    "Y_list = []\n",
    "last_col_name = \"\"\n",
    "for col in data.columns:\n",
    "    if \"Unnamed\" not in col:\n",
    "        X_list.append(data[col].values.tolist())\n",
    "        Y_list.append([float(val) for val in col.replace(\"'\", \".\").strip().split(\" \")])\n",
    "        last_col_name = col\n",
    "    else:\n",
    "        if last_col_name:\n",
    "            X_list.append(data[col].values.tolist())\n",
    "            Y_list.append(\n",
    "                [\n",
    "                    float(val)\n",
    "                    for val in last_col_name.replace(\"'\", \".\").strip().split(\" \")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "X_raw = np.array(X_list)\n",
    "\n",
    "# Apply Savitzky-Golay filter row-wise\n",
    "X_savgol = savgol_filter(X_raw, window_length=11, polyorder=3, axis=1)\n",
    "\n",
    "# Apply Standard Normal Variate (SNV) row-wise\n",
    "X_snv = standard_normal_variate(X_savgol)\n",
    "\n",
    "X = X_snv\n",
    "Y = np.array(Y_list)\n",
    "\n",
    "# =========================#\n",
    "#                          #\n",
    "#   PLS REGRESSION MODEL   #\n",
    "#                          #\n",
    "# =========================#\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Create a PLS model\n",
    "pls = PLSRegression()\n",
    "\n",
    "# Use GridSearchCV to find the best number of components using KFold cross-validation\n",
    "param_grid = {\"n_components\": list(range(1, 21))}\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(pls, param_grid, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best number of components\n",
    "best_n_components = grid_search.best_params_[\"n_components\"]\n",
    "print(f\"Best number of components: {best_n_components}\")\n",
    "\n",
    "# Fit the PLS model with the best number of components\n",
    "pls_best = PLSRegression(n_components=best_n_components)\n",
    "pls_best.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred = pls_best.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error and root mean squared error for the predictions\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "# Calculate R-squared for the predictions\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"R-squared: {r2}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "number_of_samples = 4\n",
    "\n",
    "# Display the prediction results\n",
    "Y_test_sample = Y_test[:number_of_samples]\n",
    "Y_pred_sample = Y_pred[:number_of_samples]\n",
    "\n",
    "print(\"\\nActual Concentrations (Y Test):\")\n",
    "print(Y_test_sample)\n",
    "print(\"\\nPredicted Concentrations (Y Pred):\")\n",
    "print(Y_pred_sample)\n",
    "\n",
    "\n",
    "# =============================================#\n",
    "#                                              #\n",
    "#   LEARN THE RESIDUALS USING RANDOM FOREST    #\n",
    "#                                              #\n",
    "# =============================================#\n",
    "\n",
    "\n",
    "# Calculate residuals on the training set\n",
    "residuals = Y_train - pls_best.predict(X_train)\n",
    "\n",
    "# Secondary Model: RandomForestRegressor to learn residuals\n",
    "residual_model_rf = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "residual_model_rf.fit(X_train, residuals)\n",
    "\n",
    "# Correction of PLS predictions\n",
    "Y_pred_test_pls = pls_best.predict(X_test)\n",
    "Y_residuals_pred = residual_model_rf.predict(X_test)\n",
    "Y_pred_corrected_rf = Y_pred_test_pls + Y_residuals_pred\n",
    "\n",
    "\n",
    "mse_rf = mean_squared_error(Y_test, Y_pred_corrected_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(Y_test, Y_pred_corrected_rf)\n",
    "\n",
    "print(f\"RandomForest Corrected - R-squared: {r2_rf}, MSE: {mse_rf}, RMSE: {rmse_rf}\")\n",
    "\n",
    "number_of_samples = 4\n",
    "\n",
    "# Display the prediction results\n",
    "Y_test_sample = Y_test[:number_of_samples]\n",
    "Y_pred_sample = Y_pred_corrected_rf[:number_of_samples]\n",
    "\n",
    "print(\"\\nActual Concentrations (Y Test):\")\n",
    "print(Y_test_sample)\n",
    "print(\"\\nPredicted Concentrations (Y Pred):\")\n",
    "print(Y_pred_sample)\n",
    "\n",
    "\n",
    "number_of_samples = 4\n",
    "\n",
    "# Display the prediction results\n",
    "Y_test_sample = Y_test[:number_of_samples]\n",
    "Y_pred_sample = Y_pred_corrected_rf[:number_of_samples]\n",
    "\n",
    "print(\"\\nActual Concentrations (Y Test):\")\n",
    "print(Y_test_sample)\n",
    "print(\"\\nPredicted Concentrations (Y Pred):\")\n",
    "print(Y_pred_sample)\n",
    "\n",
    "\n",
    "# =======================================================#\n",
    "#                                                        #\n",
    "#   LEARN THE RESIDUALS USING NON-NEGATIVE REGRESSION    #\n",
    "#                                                        #\n",
    "# =======================================================#\n",
    "\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "\n",
    "# Custom wrapper to enforce non-negative predictions\n",
    "class NonNegativePLS(PLSRegression):\n",
    "    def predict(self, X_train_nn):\n",
    "        Y_pred = super().predict(X_train_nn)\n",
    "        # Apply non-negative constraint\n",
    "        Y_pred_nn = np.array([nnls(np.eye(Y_pred.shape[1]), y)[0] for y in Y_pred])\n",
    "        return Y_pred_nn\n",
    "\n",
    "\n",
    "# Create a PLS model\n",
    "pls_nn = NonNegativePLS()\n",
    "\n",
    "# Use GridSearchCV to find the best number of components using KFold cross-validation\n",
    "param_grid = {\"n_components\": list(range(1, 21))}\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(pls_nn, param_grid, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best number of components\n",
    "best_n_components = grid_search.best_params_[\"n_components\"]\n",
    "print(f\"Best number of components: {best_n_components}\")\n",
    "\n",
    "# Fit the PLS model with the best number of components\n",
    "pls_nn_best = NonNegativePLS(n_components=best_n_components)\n",
    "pls_nn_best.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_nn = pls_nn_best.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error and root mean squared error for the predictions\n",
    "mse = mean_squared_error(Y_test, Y_pred_nn)\n",
    "rmse = np.sqrt(mse)\n",
    "# Calculate R-squared for the predictions\n",
    "r2 = r2_score(Y_test, Y_pred_nn)\n",
    "\n",
    "print(f\"Non Negative Regression PLS - R-squared: {r2}, MSE: {mse}, RMSE: {rmse}\")\n",
    "\n",
    "number_of_samples = 4\n",
    "\n",
    "# Display the prediction results\n",
    "Y_test_sample = Y_test[:number_of_samples]\n",
    "Y_pred_sample = Y_pred_nn[:number_of_samples]\n",
    "\n",
    "print(\"\\nActual Concentrations (Y Test):\")\n",
    "print(Y_test_sample)\n",
    "print(\"\\nPredicted Concentrations (Y Pred):\")\n",
    "print(Y_pred_sample)\n",
    "\n",
    "\n",
    "# ============================================================#\n",
    "#                                                             #\n",
    "#   LEARN THE RESIDUALS OF NNLS-PLS USING RANDOM FOREST       #\n",
    "#                                                             #\n",
    "# ============================================================#\n",
    "\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = Y_train - pls_nn_best.predict(X_train)\n",
    "\n",
    "# Secondary Model: RandomForestRegressor to learn residuals\n",
    "residual_model_rf = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "residual_model_rf.fit(X_train, residuals)\n",
    "\n",
    "# Correction of PLS predictions\n",
    "Y_pred_test_pls = pls_nn_best.predict(X_test)\n",
    "Y_residuals_pred = residual_model_rf.predict(X_test)\n",
    "Y_pred_corrected_rf = Y_pred_test_pls + Y_residuals_pred\n",
    "\n",
    "\n",
    "mse_rf = mean_squared_error(Y_test, Y_pred_corrected_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(Y_test, Y_pred_corrected_rf)\n",
    "\n",
    "print(\n",
    "    f\"Non Negative Regression RandomForest Corrected - R-squared: {r2_rf}, MSE: {mse_rf}, RMSE: {rmse_rf}\"\n",
    ")\n",
    "\n",
    "number_of_samples = 4\n",
    "\n",
    "# Display the prediction results\n",
    "Y_test_sample = Y_test[:number_of_samples]\n",
    "Y_pred_sample = Y_pred_corrected_rf[:number_of_samples]\n",
    "\n",
    "print(\"\\nActual Concentrations (Y Test):\")\n",
    "print(Y_test_sample)\n",
    "print(\"\\nPredicted Concentrations (Y Pred):\")\n",
    "print(Y_pred_sample)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
